%% LyX 1.6.7 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setcounter{tocdepth}{1}
\usepackage{babel}

\usepackage{amsmath}
\usepackage[unicode=true, pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\providecommand{\LyX}{L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\@}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{clrscode3e}

\makeatother

\begin{document}

\title{Compilers: notes for UIC qual%
\thanks{This work is licensed under a \underbar{\protect\href{http://creativecommons.org/licenses/by-nc-sa/3.0}{Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License}}.%
}}


\author{Gugo}

\maketitle

\subsection*{Disclaimer}

These notes have been prepared with the \textbf{only} purpose to help
me pass the Computer Science qualifiying exam at the University of
Illinois at Chicago. They are distributed as they are (including errors,
typos, omissions, etc.) to help other students pass this exam (and
possibly relieving them from part of the pain associated with such
a process). I take \textbf{no responsibility} for the material contained
in these notes (which means that you can't sue me if you don't pass
the qual!) Moreover, this pdf version is distributed together with
the original \LaTeX{} (and \LyX{}) sources hoping that someone else
will improve and correct them. I mean in absolute no way to violate
copyrights and/or take credit stealing the work of others. The ideas
contained in these pages are \textbf{not mine} but I've just aggregated
information scattered all over the internet.


\subsection*{Strongly suggested book}

Before you even start reading this notes, do yourself a favor and
go buy the {}``Dragon Book''. \\
(\href{http://dragonbook.stanford.edu/}{http://dragonbook.stanford.edu/})
It is the {}``bible'' of compilation and covers all the topics you
need to know to pass the qualifier very deeply and extensively.

\tableofcontents{}


\section{Overall compiler structure}

See diagrams on p. 2-7, p.106 of the dragon book.

Note how an \emph{interpreter}, instead of producing a target program
as a translation, directly executes the operations specified in the
source program on inputs supplied by the user. The machine-language
target program produced by a compiler is usually much faster than
an interpreter at mapping inputs to outputs. (This is why, for instance,
Java uses Just In Time compilation improve its execution speed) An
interpreter, however can usually give better error diagnostics than
a compiler, because it executes the source program statement by statement.


\section{Lexical analysis (Scanning)}

The main task of a lexical analyzer is to read the input characters
of the source program, group them into lexemes, and produce as output
a sequence of tokens for each lexeme in the source program. The stream
of token is then sent to the parser for syntax analysis.

Commonly the parser calls the lexical analyzer and such a call causes
the lexer to read characters from its input until it can identify
the next lexeme and produce for it the next token, which is then returned
to the parser. (see Fig. 3.1, p. 110)

Since the lexer is the part of the compiler that reads the source
text, it may perform certain other tasks besides identification of
lexemes. One such task is stripping out whitespaces (blank, tab and
newline) and comments. Another task is correlating error messages
generated by the compiler with the source program. For instance, the
lexical analyzer may keep track of the number of newline characters
seen, so it can associate a line number with each error message. If
the source program uses a macro-processor, the expansion of macros
may also be performed by the lexical analyzer.


\subsection{Tokens}

In many programming languages, the following classes cover most or
all of the tokens:
\begin{itemize}
\item One token for each keyword ($\func{if},\func{else},\ldots$)
\item Tokens for the operators, either individually or in a class (e.g.
$\func{comparison}$ vs $=,<,\geq,\ldots$)
\item One token representing all the identifiers ($\func{id}$)
\item One or more token representing constants, such as numbers and literal
strings ($\func{number},\func{literal},\ldots$)
\item Tokens for each punctuation symbol, such as left and right parentheses,
comma and semicolon ($\func{lp},\func{rp},\ldots$)
\end{itemize}
When more than one lexeme can mach a pattern, the lexical analyzer
must provide subsequent phases of the compiler additional information
about the particular lexeme that matched. (e.g. any number matches
the $\func{number}$ token but any number has a different $\func{value}$
attribute) Thus, in many cases the lexical analyzer returns to the
parser not only a token name, but an attribute value that describes
the lexeme represented by the token. The token name influences parsing
decisions, while the attribute value influences translation of tokens
after the parse.

We shall assume that tokens have at most one associated attribute,
although this attribute may have a structure that combines several
pieces of information. The most important example is the token $\func{id}$,
where we need to associate with the token a great deal of information
which is normally stored in the symbol table. Thus, the appropriate
attribute value for an identifier is a pointer to the symbol table
entry for that identifier.


\subsection{Lexical errors}

It is hard for a lexical analyzer to tell, without the aid of other
components, that there is a source code error. However, suppose a
situation arises where the lexical analyzer is unable to proceed because
none of the patterns for tokens matches any prefix of the remaining
input. 

The simplest recovery strategy is {}``\emph{panic mode}'' recovery.
We delete successive characters from the remaining input, until the
lexical analyzer can find a well-formed token at the beginning of
what input is left. Other possible error recovery actions are: delete
one character from the remaining input; insert a missing character
into the remaining input; replace a character with another character;
transpose two adjacent characters. Transformations like these can
be tried in an attempt to repair the input.


\subsection{Input buffering}

Specialized buffering techniques have been developed to reduce the
amount of overhead required to process a single input character. An
important scheme involves two (contiguous) buffers that are alternatively
reloaded. Each buffer is of the same size $N$, where $N$ is usually
the size of a disk block. Using a system read command we can read
$N$characters into the buffer, rather than using one system call
per character. If fewer than $N$ characters remain in the input file,
then a special character, represented by $\func{eof}$ and different
from any possible character in the source program, marks the end of
the source file. Two pointers to the input buffer are maintained:
\begin{itemize}
\item pointer \texttt{lexemeBegin}, marks the beginning of the current lexeme,
whose extent we are trying to determine;
\item pointer \texttt{forward} scans ahead until a pattern match is found.
\end{itemize}
When parsing starts both pointers are set to point to the first character
in the input buffer. Once the next lexeme is determined, \texttt{forward}
is set to the character at the lexeme's right end and, after the lexeme
is recorded as an attribute value of a token returned to the parser,
\texttt{lexemeBegin} is set to the same value of \texttt{forward}. 

Advancing \texttt{forward} requires that we first test whether we
have reached the end of one of the buffers, and if so, we must reload
the other buffer from the input, and move \texttt{forward} to the
beginning of the newly loaded buffer. As long as we never need to
look so far ahead of the actual lexeme that the sum of the lexeme's
length plus the distance we look ahead is greater than $N$ we should
never overwrite the lexeme in its buffer before determining it.


\subsection{DFAs}

See DFAs section in {}``Theory of Computation'' notes!

To build a lexical analyzer we first express patterns using \emph{regular
expressions}, we then convert regular expressions to \emph{transition
diagrams} (DFAs) and finally implement a program that simulates input
on the DFA and decides to accept/reject the string.


\section{Symbol tables management}

\emph{Symbol tables} are data structures that are used by the compilers
to store information about source-program constructs. Such data structure
should be designed to allow a compiler to find the record for each
construct quickly and to store or retrieve data from that record quickly.
The information is collected incrementally by the analysis phases
of a compiler (front-end) and used by the synthesis phase (back-end)
to generate the target code.

Entries in the symbol table contain information about an identifier
such as its lexeme, its type, its position in storage, and any other
relevant information. In some cases, lexical analyzers can create
symbol table entries as soon as they see the characters that make
up a lexeme. More often, the lexical analyzer can only return to the
parser a token along with a pointer to the lexeme. only the parser,
however, can decide whether to use a previously created symbol table
entry or create a new one for the identifiers.

Since symbol tables typically need to support multiple declarations
of the same identifier within a program we shall implement scopes
by setting up separate symbol tables for each scope (\emph{nested
symbol tables}). A program block with declarations will have its own
symbol table and with an entry for each declaration in the block.
This approach also works for other constructs that set up scopes:
for example, a class would have its own table, with an entry for each
field and method.


\section{CFGs}
\begin{itemize}
\item See CFGs section in {}``Theory of Computation'' notes!
\item If $S\overset{+}{\Rightarrow}\alpha$, where $S$ is the start symbol
of the grammar $G$, we say that $\alpha$ is a \emph{sentential form}.
\item There is a many-to-one relationship between derivations and parse
trees. Every parse trees has associated with it a unique leftmost
and rightmost derivation.
\end{itemize}

\subsection{Eliminating ambiguity}

Ambiguous grammars can be used sometimes together with a set of disambiguating
rules that {}``throw away'' undesired parse trees leaving only one.
(e.g. the {}``dangling else'' ambiguity can be solved with the rule
{}``match each else with the closest unmatched then''%
\footnote{Another important example of ambiguity elimination through rules is
the use of associativity and precedence to resolve conflicts in arithmetic
expressions.%
})


\subsection{Eliminating Left Recursion}

A grammar is \emph{left-recursive} if it has a nonterminal $A$ such
that there is a derivation $A\overset{+}{\Rightarrow}A\alpha$. 

Top down parsing methods cannot handle left-recursive grammars, so
a transformation is needed to eliminate left recursion. The following
procedure takes as input a grammar $G$ with \emph{no} cycles or $\epsilon$-productions
and returns an equivalent grammar with no left recursion.

\begin{codebox}
\Procname{\proc{Eliminate-Left-Recursion}($G$)}
\li arrange the nonterminals in some order $A_1,A_2,\ldots,A_n$
\li \For $j=1$  \To $i-1$ \Do
\li    replace each production of the form $A_i\rightarrow A_j\gamma$ by the
\zi    productions 
       $A_i\rightarrow \delta_1\gamma|\delta_2\gamma|\ldots|\delta_k\gamma$, where
\zi    $A_j\rightarrow \delta_1|\delta_2|\ldots|\delta_k$ are all 
       current $A_j$-productions 
    \End
\li eliminate the immediate left recursion among the $A_j$-productions 
\end{codebox}


\subsection{Left factoring}

Left factoring is a grammar transformation that is useful for producing
a grammar suitable for predictive (or top-down) parsing. The following
procedure takes as input a grammar $G$ and returns an equivalent
left-factored grammar.

\begin{codebox}
\Procname{\proc{Left-Factoring}($G$)}
\li \For each nonterminal  $A$ \Do
\li    find the longest common prefix $\alpha$ common to two (or more) of its alternatives
\li   \If $\alpha \neq \epsilon$ \Then
\li     replace all
        $A\rightarrow\alpha\beta_1|\alpha\beta_2|\ldots|\alpha\beta_n|\gamma$ with
\li     $A\rightarrow\alpha A'|\gamma$ and $A'\rightarrow\beta_1|\beta_2|\ldots|\beta_n$
      \End
    \End
\end{codebox}


\subsection{Non-CFG constructs}

A few syntactic constructs found in typical programming languages
cannot be specified using grammars alone. Typical examples are:
\begin{itemize}
\item the language that abstracts the problem of checking that identifiers
are declared before they are used in a program;
\item the language that abstracts the problem of checking that the number
of formal parameters in the declaration of a function agrees with
the number of actual parameters in a use of the function.
\end{itemize}
These construct are checked in later phases of compilation and in
particular using semantic analyzer. 


\section{Parsing (Syntax analysis)}


\subsection{Recursive-Descent Parsing}

A \emph{recursive-decent} parsing program consists of a series of
routines, one for each non terminal. Execution begins with the procedure
for the start symbol, which halts and announces success if its procedure
body scans the entire input string. General recursive-descent parsing
may require backtracking; that is, it may require repeated scans over
the input.


\subsection{\proc{First} and \proc{Follow}}


\subsubsection{\proc{First}}

To compute \proc{First}($X$) for all grammar symbols $X$, apply
the following rules until no more terminals or $\epsilon$ can be
added to any \proc{First} set.
\begin{enumerate}
\item If $X$ is a terminal, then the \proc{First}($X$)$={X}$.
\item If $X$ is a nonterminal and $X\rightarrow Y_{1}Y_{2}\ldots Y_{k}$
is a production for some $k\geq1$, then place $a$ in \proc{First}($X$)
if for some $i$, $a$ is in \proc{First}($Y_i$) and $\epsilon$
is in all of \proc{First}($Y_1$),\ldots,\proc{First}($Y_{i-1}$);
that is, $Y_{1}\ldots Y_{i-1}\overset{*}{\Rightarrow}\epsilon$. If
$\epsilon$ is in \proc{First}($Y_j$) for all $j=1,2,\ldots,k$,
then add $\epsilon$ to \proc{First}($X$). 
\item If $X\rightarrow\epsilon$ is a production, then add $\epsilon$ to
\proc{First}($X$).
\end{enumerate}

\subsubsection{\proc{Follow}}

To compute \proc{Follow}($A$) for all nonterminals $A$, apply the
following rules until nothing can be added to any \proc{Follow} set.
\begin{enumerate}
\item Place \$ in \proc{Follow}($S$), where $S$ is the start symbol, and
\$ is the input right end marker.
\item If there is a production $A\rightarrow\alpha B\beta$, then everything
in \proc{First}($\beta$) except $\epsilon$ is in \proc{Follow}($B$).
\item If there is a production $A\rightarrow\alpha B$ (or a production
$A\rightarrow\alpha B\beta$ where \proc{First}($\beta$) contains
$\epsilon$) then everything is \proc{Follow}($A$) is in \proc{Follow}($B$)
\end{enumerate}

\subsection{Predictive Parsing - LL(1)}

\emph{Predictive parsers}, that is, recursive-descent parsers needing
no backtracking, can be constructed for a class of grammars called
LL(1). The first {}``L'' stands for scanning the input from left
to right, the second {}``L'' for producing a leftmost derivation,
and the {}``1'' for using one input symbol of lookahead at each
step to make parsing action decisions.

A grammar $G$ is LL(1) if and only if whenever $A\rightarrow\alpha|\beta$
are two distinct productions of $G$, the following conditions hold: 
\begin{enumerate}
\item For no terminal $a$ do both $\alpha$ and $\beta$ derive strings
beginning with $a$.
\item At most one of a $\alpha$ or $\beta$ can derive the empty string.
\item If $\beta\overset{*}{\Rightarrow}\epsilon$, then $\alpha$ does not
derive any string beginning with terminal in \proc{Follow($A$)}.
Likewise, if $\alpha\overset{*}{\Rightarrow}\epsilon$, then $\beta$
does not derive any string beginning with terminal in \proc{Follow($A$)}.
\end{enumerate}
The first two conditions are equivalent to the statement that \proc{First}($\alpha$)
and \proc{First}($\beta$) are disjoint sets. The third condition
is equivalent to stating that, if $\epsilon$ is in \proc{First}($\beta$)
, then \proc{First}($\alpha$) and \proc{Follow}($A$) are disjoint
sets, and likewise if $\epsilon$ is in \proc{First}($\alpha$).


\subsubsection{Parsing table construction}

The following algorithm takes a LL(1) grammar $G$ and returns the
parsing table, $M[N,t]$ where $N$ is a nonterminal and $t$ a terminal,
necessary to build the parser.
\begin{enumerate}
\item For each production $A\rightarrow\alpha$ of the grammar, do the following:

\begin{enumerate}
\item For each terminal $a$ in \proc{First}($A$), add $A\rightarrow\alpha$
to $M[A,a]$.
\item If $\epsilon$ is in \proc{First}($\alpha$), then for each terminal
$b$ in \proc{Follow($A$)}, add $A\rightarrow\alpha$ to $M[A,b]$.
If $\epsilon$ is in \proc{First}($\alpha$) and \$ is in \proc{Follow($A$)},
add $A\rightarrow\alpha$ to $M[A,\text{\$}]$ as well.
\end{enumerate}
\end{enumerate}

\subsubsection{Parser}

The following algorithm takes as input a string $w$ and a parsing
table $M$ for grammar $G$ and returns a leftmost derivation of $w$,
if $w\in L(G)$, or an error, otherwise.

Initially the parser is in a configuration with $w$\$ in the input
buffer and $S$\$ on the stack.

\begin{codebox}
\Procname{\proc{Predictive-Parsing}($M,w$)}
\li $ip \gets w[1]$
\li $X \gets S$
\li \While $X \neq \text{\$}$ \Do
\li   \If $X \isequal ip$ \Then
\li      pop the stack and advance $ip$ 
\li   \ElseIf $X$ is a terminal \Then
\li      \Error
\li   \ElseIf $M[X,a]$ is an error entry \Then
\li      \Error
\li   \ElseIf $M[X,a] \isequal X\rightarrow Y_1Y_2\ldots Y_k$ \Then
\li      output the production
\li      pop the stack
\li      push $Y_k,Y_{k-1},\ldots ,Y_1$ onto the stack
      \End
    \End 
\end{codebox}


\subsection{Shift-Reduce Parsing}

Shift-reduce parsing is a form of bottom-up parsing in which, during
left-to-right scan of the input string, the parser shifts zero or
more input symbols onto the stack, until it is ready to reduce a string
$\beta$ to the head of the appropriate production.

The following algorithm, takes as input a string $w$ and a grammar
$G$ and returns accept if $w\in L(G)$, and an error, otherwise.
Initially the parser is in a configuration with $w$\$ in the input
buffer and an empty stack (\$ is also used to mark the bottom of the
bottom of the stack)

\begin{codebox}
\Procname{\proc{Shitf-Reduce-Parsing}($G,w$)}
\li $ip \gets w[1]$
\li \While $\attrib{stack}{top} \neq S$ or $ip \neq \text{\$}$ \Do
\li   \If $\attrib{stack}{top} \isequal \alpha$ such that $A\rightarrow\alpha$ is in $G$  \Then
\li      \proc{Reduce}
\li   \Else \proc{Shift}  
     \End
    \End 
\li \If $\attrib{stack}{top} \isequal S$ \Then
\li    \proc{Accept} 
\li  \Else \proc{Error} \End
\end{codebox}

The algorithms relies on four procedures whose behavior is intuitively
described below.
\begin{description}
\item [{Shift:}] Shifts the next symbol onto the top of the stack.
\item [{Reduce:}] The right end of the string to be reduced must be at
the top of the stack. Locate the left end of the string within the
stack and decide with what nonterminal to replace the string.
\item [{Accept:}] Announce successful completion of parsing.
\item [{Error:}] Discover a syntax error and call an error recovery routine.
\end{description}

\subsubsection{Conflicts}
\begin{description}
\item [{Shift-reduce~conflict:}] knowing the entire stack content and
the next input symbol, the parser cannot decide whether to shift or
to reduce. (e.g. $G:S\rightarrow1S,S\rightarrow1$, $w="111"$, after
shifting the first {}``$1$'' the parser doesn't know if it has
to reduce with $S\rightarrow1$ or shift again)
\item [{Reduce-reduce~conflict:}] knowing the entire stack content and
the next input symbol, the parser cannot decide which of several reductions
to make. (e.g. $G:S\rightarrow A1,S\rightarrow B2,A\rightarrow1,B\rightarrow1$,
$w="111"$, after shifting the first {}``$1$'' the parser doesn't
know if it has to reduce with $A\rightarrow1$ or $B\rightarrow1$)
\end{description}
It can be shown that when these conflict happen the grammar is not
an LR(0) grammar. Both conflicts can be solved by Simple LR parsers.


\subsection{Simple LR (SLR) Parsing}

The most prevalent type of bottom-up parsers is based on a concept
called LR(k) parsing: the {}``L'' is for left-to-right scanning
of the input, the {}``R'' for constructing the rightmost derivation
in reverse, and the {}``k'' for the number of input symbols of lookahead
that are used in making parsing decisions. The SLR or \emph{LR(0)}
parser is the simplest of this family.


\subsubsection{LR(0) automaton}

The first step to build a LR(0) parser is to build the so called \emph{LR(0)
automaton}, which is simply a DFA whose set of states is called canonical
LR(0) collection. In order to so we first compute the \emph{augmented
grammar} $G'=(S'\rightarrow S)\cup G$. The purpose of this new starting
production is to indicate to the parser when it should stop parsing
and announce acceptance of the input.

The following algorithm takes as input $G'$ and returns the \emph{canonical
LR(0) collection }$C$ using the procedures $\proc{Closure}$ and
$\proc{Goto}$ described below.

\begin{codebox}
\Procname{\proc{LR0-Automata-Build}($G'$)}
\li $C \gets \proc{Closure}(\{ S' \rightarrow\cdot S \})$
\li \Repeat
\li    \For each set of items $I\in C$ \Do
\li       \For each grammar symbol $X$ in $I$ \Do
\li          \If \proc{Goto}($I,X$) is not empty and not in $C$ \Then
\li            $C \gets C \cup \proc{Goto}(I,X)$ \End
          \End
       \End
\li \Until no new sets of items are added to $C$
\li return C
\end{codebox}

The following algorithm takes as input $G'$ and returns the set of
items $A$ which represents the outcome of the transition function
$\proc{Goto}$ for the LR(0) automata from state $I$ with input $X$.

\begin{codebox}
\Procname{\proc{Goto}($G',I,X$)}
\li \If $\{ A\rightarrow\alpha\cdot X \beta \}\in I$ \Then
\li    \Return \proc{Closure}($G',\{ A\rightarrow\alpha X \cdot \beta \}$)
\end{codebox}

The following procedure takes as input $G'$ and returns the closure
of the items set $I$.

\begin{codebox}
\Procname{\proc{Closure}($G',I$)}
\li $J \gets I$
\li \Repeat
\li    \For each item $A\rightarrow\alpha\cdot B\beta$ in $J$ \Do
\li       \For each production $B\rightarrow\gamma$ in $G$ \Do
\li          \If $B\rightarrow\cdot\gamma$ is not in  $J$ \Then
\li              add $B\rightarrow\cdot\gamma$ to $J$ \End
          \End
       \End
\li \Until no more items are added to $J$
\li \Return $J$
\end{codebox}

Note how, by construction, each state of the LR(0) automaton has a
corresponding grammar symbol. States correspond to sets of items,
and \emph{all transitions to a particular state must be for the same
grammar symbol}. Thus each state, except the start state $0$, has
a unique grammar symbol associated with it.


\subsubsection{Parsing tables}

The next step necessary to obtain a SLR parser for a grammar $G'$
is to build \emph{action} and \emph{goto} tables. If, after creating
the tables, any conflicting action result, we say that the grammar
is not SLR(1).


\paragraph{Action table}

The value of $Action[i,a]$ where $i$ is a state of the LR(0) automaton
and $a$ is a terminal (or \$) are determined as follows:
\begin{enumerate}
\item If $\{A\rightarrow\alpha\cdot a\beta\}\in I_{i}$ and $\proc{Goto}(I_{i},a)=I_{j}$
then $Action[i,a]="\func{shift}(j)"$.
\item If $\{A\rightarrow\alpha\cdot\}\in I_{i}$ and $A\neq S'$ then $\forall a\in\proc{Follow}(A),Action[i,a]="\func{reduce}(\{A\rightarrow\alpha\})"$.
\item If $\{S\rightarrow S'\cdot\}\in I_{i}$ then $Action[i,\text{\$}]="\func{accept}"$.
\item Otherwise $Action[i,a]="\func{error}"$.
\end{enumerate}

\paragraph{Goto table}

Set $Goto[i,A]=j$, where $i$ is a state of the LR(0) automaton,
$A$ is a nonterminal and $j$ is the state corresponding to $\proc{Goto}(I_{i},A)=I_{j}$.


\subsubsection{LR-Parsing algorithm}

The following algorithms takes as input an input string $w$\$ and
the $Action$ and $Goto$ parsing tables for a grammar $G$ and returns
the reduction steps of a bottom-up parse for $w$, if $w\in L(G)$,
an error, otherwise.

Note how the initial state of the parser is the one constructed from
the set of items containing $\{S\rightarrow\cdot S'\}$ and labeled
as $s_{0}$. We assume $s$ to be the state on top of the stack.

\begin{codebox}
\Procname{\proc{LR-Parsing}($G',I$)}
\li $s \gets s_0$   
\li $a \gets w[1]$
\li \While 1 \Do
\li   \If $Action[s,a] \isequal \func{shift}(t)$ \Then
\li      push $t$ onto the stack
\li      let $a$ be the next input symbol
\li   \ElseIf $Action[s,a] \isequal \func{reduce}(\{A\rightarrow\beta\})$ \Then
\li      pop $|\beta|$ symbols off the stack, which implies $s \gets r$
\li      push $\proc{Goto}(s,A)$ onto the stack
\li      $\func{print}(A\rightarrow\beta)$
\li   \ElseIf $Action[s,a] \isequal \func{accept}$ \Then
\li      \Return
\li   \Else \func{error} \End
    \End
\end{codebox}

Note that \emph{all} LR parsers behave in this fashion; the only difference
between one LR parser and another is the information stored in the
$Action$ and $Goto$ parsing tables.


\subsection{LR(1) Parsing}

This method is called the \emph{canonical-LR} or just \emph{LR} method
and makes full use of the lookahead symbol(s). The set of items for
this method is large and is called the \emph{LR(1) items}.


\subsubsection{LR(1) automaton}

The general form of an \emph{LR(1) item} becomes $[A\rightarrow\alpha\cdot\beta,a]$
where $A\rightarrow\alpha\beta$ is a production and $a$ is a terminal
or the right end marker $\$$. The second element is called the \emph{lookahead}.
The lookahead affects only the items where $\beta$ is $\epsilon$
or have the form $[A\rightarrow\alpha\cdot,a]$. In this case such
an item calls for a reduction only if the next input symbol is $a$.

The method for building the collection of sets of a valid LR(1) items
is essentially the same as the one for building the canonical collection
of sets of LR(0) items. We need only to modify the following procedures.

\begin{codebox}
\Procname{\proc{LR0-Automata-Build}($G'$)}
\li $C \gets \proc{Closure}(\{ [S' \rightarrow\cdot S,\$] \})$
\li \Repeat
\li    \For each set of items $I\in C$ \Do
\li       \For each grammar symbol $X$ in $I$ \Do
\li          \If \proc{Goto}($I,X$) is not empty and not in $C$ \Then
\li            $C \gets C \cup \proc{Goto}(I,X)$ \End
          \End
       \End
\li \Until no new sets of items are added to $C$
\li \Return C
\end{codebox}

\begin{codebox}
\Procname{\proc{Goto}($G',I,X$)}
\li $J \gets \emptyset$
\li \For each  item $[A\rightarrow\alpha\cdot X \beta ,a] \in I$ \Do
\li    add item $[A\rightarrow\alpha X \cdot \beta ,a]$ to set $J$ \End
\li    \Return \proc{Closure}($J$)
\end{codebox}

\begin{codebox}
\Procname{\proc{Closure}($G',I$)}
\li $J \gets I$
\li \Repeat
\li    \For each item $[A\rightarrow\alpha\cdot B\beta ,a]$ in $J$ \Do
\li       \For each production $B\rightarrow\gamma$ in $G'$ \Do
\li          \For each terminal $b$ in \proc{First}($\beta a$) \Do
\li              add $[B\rightarrow\cdot\gamma ,b]$ to $J$ \End
          \End
       \End
\li \Until no more items are added to $J$
\li \Return $J$
\end{codebox}


\subsubsection{Parsing tables}

Again, we just modify the procedure to build the $Action$ table to
adapt to LR(1) parsing.


\paragraph{Action table}

The value of $Action[i,a]$ where $i$ is a state of the LR(1) automaton
and $a$ and $b$ are terminals (or \$) are determined as follows:
\begin{enumerate}
\item If $[A\rightarrow\alpha\cdot a\beta,b]$ is in $I_{i}$ and $\proc{Goto}(I_{i},a)=I_{j}$
then set $Action[i,a]$ to {}``shift $j$''. Here $a$ must be a
terminal.
\item If $[A\rightarrow\alpha\cdot,a]$ is in $I_{i}$, $A\neq S'$, then
set $Action[i,a]$ to {}``reduce $A\rightarrow\alpha$''. 
\item If $[S'\rightarrow S\cdot,\$]$ is in $I_{i}$ then set $Action[i,a]$
to {}``accept''. 
\item Otherwise $Action[i,a]="\func{error}"$.
\end{enumerate}
If any conflicting actions result from the above rules, we say the
grammar is not LR(1) and we can't produce a parser.


\paragraph{Goto table}

Same as LR(0) automaton.


\subsubsection{LR-Parsing algorithm}

As said before the parsing algorithm is the same one used in SLR parsers.


\subsection{LALR(1) Parsing}

The \emph{lookahead-LR} or \emph{LALR} method is based on the LR(0)
set of items and has many fewer states than typical parsers based
on LR(1) items. By carefully introduction lookaheads into the LR(0)
items, we can handle many more grammars than with SLR, and build parsing
tables that are no bigger than SLR tables. LALR is the method of choice
in most situations.


\subsection{Error handling}

It is desirable for a parser to report the presence of errors clearly
and accurately, recover from each error quickly enough to detect subsequent
errors and add a minimal overhead to the processing of correct programs.

The following are four of the most useful error recovery strategies.


\paragraph{Panic-Mode Recovery}

The parser discards input symbols one at the time until one of a designated
set of s\emph{ynchronizing tokens} is found. These are usually delimiters,
such as ';' and '\}'.


\paragraph{Phrase-Level Recovery}

On discovering an error, a parser may perform local correction on
the remaining input; that is, it may replace a prefix of the remaining
input by some string that allows the parser to continue. (e.g. replace
',' with ';') The major drawback is the difficulty it has in coping
with situations in which the actual error has occurred before the
point of detection.


\paragraph{Error Productions}

By anticipating common errors that might be encountered, we can augment
the grammar for the language with productions that generate the erroneous
constructs.


\paragraph{Global Correction}

Ideally, we would like a compiler to make as few changes as possible
in processing an incorrect input string. There are algorithms for
choosing a minimal sequence of changes to obtain a globally least-cost
correction. Unfortunately, these methods are in general too costly
to implement in terms of time and space, so these techniques are currently
only of theoretical interest.


\section{Syntax-directed Translation}

Syntax-directed Translation is done by attaching rules or program
fragments to productions in a grammar. There are two different strategies
to accomplish this. 


\subsection{Attributes and Syntax Directed Definitions}

An \emph{attribute} is any quantity associated with a programming
construct. Since we use grammar symbols to represent programming constructs
we extend the notion of attributes from constructs to the symbols
that represent them.

A \emph{syntax-directed definition} (\emph{SDD}) associates with each
grammar symbol, a set of attributes, and with each production, a set
of \emph{semantic rules} for computing the values of the attributes
associated with the symbols appearing in the production. 

An attribute is said to be \emph{synthesized} if its value at a parse-tree
node $N$ is determined from attribute values at the children of $N$
and $N$ itself. Synthesized attributes have the desirable property
that they can be evaluated during a single bottom-up traversal of
the \emph{annotated parse tree}.

An attribute is said to be \emph{inherited} if its value at a parse-tree
node $N$ is defined only in terms of attribute values at $N$'s parent,
$N$ itself and $N$'s siblings.

Terminals can have synthesized attributes, but not inherited attributes.
Attributes for terminals have lexical values that are supplied by
the lexical analyzer; there are no rules in the SDD itself for computing
the value of an attribute for a terminal.


\subsubsection{Evaluation orders for SDDs}

In order to decide in which order to evaluate the attributes we need
to build a \emph{dependency grap}h. If the dependency graph has an
edge from node $M$ to node $N$, then the attributes corresponding
to $M$ must be evaluated before the attributes of $N$. The \emph{topological
sort} of the dependency graph leads to a sequential ordering of evaluation
for the attributes. Clearly, if there is any cycle in the dependency
graph it is impossible to find such a topological order. (see p. 338)


\paragraph{S-attributed SDDs}

An SDD that involves only synthesized attributes is called \emph{S-attributed}.
In such SDD, each rule computes an attribute for the nonterminal at
the head of the production from attributes taken from the body of
the production. An S-attributed SDD can be implemented naturally in
conjunction with an LR parser, since a bottom-up parser corresponds
to a postorder traversal. This strategy can be used to store the evaluated
values for synthesized attributes on the stack during LR parsing,
without creating the tree nodes explicitly.


\paragraph{L-attributed SDDs}

An SDD is called \emph{L-attributed} if every attribute is either
synthesized or, if it is inherited, it depends only on inherited attributes
of its parent and on (any) attributes of siblings to its left. More
precisely, suppose that there is a production $A\rightarrow X_{1}X_{2}\ldots X_{n}$,
and that there is an inherited attribute \attrib{X_i}{a} computed
by a rule associated with this production. Then the rule may only
use: 
\begin{enumerate}
\item Inherited attributes associated with $A$
\item Either inherited or synthesized attributes associated with the occurrences
of symbols $X_{1},X_{2},\ldots,X_{i-1}$ located to the left of $X_{i}$.
\item Inherited or synthesized attributes associated with this occurrence
of $X_{i}$ itself, but only in such a way that there are no cycles
in a dependency graph formed by the attributes of this $X_{i}$.
\end{enumerate}

\subsubsection{Semantic rules and side effects}

Side effects are other actions, like printing the value of an attribute,
that are executed during Syntax Directed Translations. 

An SDD without side effects is sometimes called\emph{ attribute grammar}.
The rules in an attribute grammar define the value of an attribute
purely in terms of the values of other attributes and constants. 

Usually we allow controlled side effects; that is, we permit side
effects that do not constrain attribute evaluation. 


\subsubsection{Abstract syntax tree construction using SDDs}

A typical application of SDDs is the construction of syntax trees,
which can be then used as an internal representation usually thought
the $.ast$ attribute. Clearly, to complete the translation to intermediate
code, the compiler may then walk the syntax tree, using another set
of rules that are in effect an SDD on the syntax tree rather than
the parse tree.


\subsection{Semantic actions and Syntax Directed Translation Schemes (SDT)}

A \emph{translation scheme} is a notation for attaching program fragments
to the production of a grammar. The program fragments, called \emph{semantic
actions}, are executed when the production is used during syntax analysis. 

The position at which an action is to be executed is shown by enclosing
it between curly braces and writing it within the production body.
When drawing a parse tree for a translation scheme, we indicate an
action by constructing an extra child for it, connected by a dashed
line in the node that corresponds to the head of the production. Any
STD can be implemented by first building a parse tree and then performing
the actions in a left-to-right depth first order; that is, a \emph{preorder
traversal}.

Typically, SDTs are implemented during parsing, without building a
parse tree. SDT that can be implemented during parsing can be characterized
by introducing distinct \emph{marker nonterminals} in place of each
embedded action; each marker $M$ has only one production, $M\rightarrow\epsilon$.
If the grammar with the marker nonterminals can be parsed by a given
method (LL, LR,...), then the SDT can be implemented during parsing. 


\subsection{Implementing SDDs using SDTs}

SDTs are often used to implement SDDs in real compilers.


\subsubsection{SDTs for LR-parsable grammars and S-attributed SDD}

We can construct an SDT in which each action is placed at the end
of the production and is executed along with the reduction of the
body to the head of that production. SDTs with all actions at the
ends of the production body are called \emph{postfix SDTs}.


\subsubsection{SDTs for LL-parsable grammars and L-attributed SDD}

The rules for turning an L-attributed SDD, whose underlying grammar
is LL-parsable, into an SDT are as follow:
\begin{enumerate}
\item Embed the action that computes the inherited attributes for a nonterminal
$A$ immediately before that occurrence of $A$ in the body of the
production. If several inherited attributes for $A$ depend on one
another in an acyclic fashion, order the evaluation of attributes
so that those needed first are computed first.
\item Place the actions that compute a synthesized attribute for the head
of a production at the end of the body of that production
\end{enumerate}

\subsubsection{L-attributed SDDs and recursive-descent parsing}

TODO p.338


\subsubsection{L-attributed SDDs and LL parsing}

TODO p.343


\subsubsection{L-attributed SDDs and LR parsing}

TODO p.348 


\section{Semantic Analysis and Intermediate representation}


\subsection{Type checking}

TODO


\subsection{Intermediate representation}

The purpose of a compiler frontend is to construct an intermediate
representation of the source program from which the back end generates
the target program. The two most important intermediate representations
are:
\begin{description}
\item [{Trees:}] Including \emph{parse trees} and \emph{Abstract Syntax
Trees} (\emph{AST})%
\footnote{Syntax trees resemble parse trees to an extent; however, in the syntax
tree the internal node represent programming constructs while in the
parse tree, the interior nodes represent nonterminals. Many nonterminals
of a grammar represent programming constructs, but others are {}``helpers''
of some sort. (e.g. see Fig. 2.5, p. 47 and Fig.2.22, p. 70) In the
syntax tree, these helpers typically are not needed and hence dropped.%
}. During parsing, syntax-tree nodes are created to represent significant
programming constructs. As analysis proceeds, information is added
to the nodes in the form of attributes associated with nodes.
\item [{Linear~representations:}] especially \emph{three-address code}.
This is just a sequence of elementary program instructions, such as
the addition of two values.
\end{description}
It is possible that a compiler will construct a syntax tree at the
same time it emits three-address codes. However, it is common for
a compiler to emit three-address code while the parser {}``goes through
the motions'' of constructing a syntax tree'' without actually constructing
the complete tree data structure. Rather, the compiler stores nodes
and their attributes needed for semantic analysis or other purposes,
along with the data structure used for parsing (i.e. the parse tree).
By doing so, those parts of the syntax tree that are needed to construct
the three-address code are available when needed, but disappear when
no longer needed.


\section{Code generation}

The code generator takes as input the intermediate representation
(IR) produced by the front end of the compiler, along with relevant
symbol table information, and produces as output a semantically equivalent
(and correct) target program. The problem of generating an optimal
target program for a given source program is undecidable, and many
of the subproblems encountered during code generation such as register
allocation are computationally intractable. The three main tasks of
a code generator are:
\begin{itemize}
\item \emph{Instruction selection} The nature of the instruction set of
the target architecture has a strong effect on the difficulty of this
task. Also, if we don't care about the efficiency of the target program
instruction selection is straightforward: on most machines, a given
IR program can be implemented by many different code sequences, with
significant cost differences between the different implementations.
\item \emph{Register allocation and assignment} Register allocation is the
process of selecting the set of variables that will reside in registers
at each point in the problem. Register assignment is the process of
picking the specific register that a variable will reside in. This
task is NP-complete.
\item \emph{Instruction ordering} The order in which computations are performed
can affect the efficiency of the target code. Picking an optimal order
in the general case is a difficult NP-complete problem.
\end{itemize}

\subsection{Basic blocks and flow graph}

In order to represent three-address instructions as a flow graph we
need to first partition them into basic blocks. Basic blocks are maximal
sequences of consecutive three-address instructions with the property
that:
\begin{itemize}
\item The flow of control can only enter the basic block through the first
instruction in the block. That is, there are no jumps into the middle
of the block.
\item Control will leave the block without halting or branching, except
possibly at the last instruction in the block.
\end{itemize}
The following algorithm accomplishes such a task:
\begin{enumerate}
\item Find all \emph{leaders}, that is, the first instructions in some basic
block. The rules for finding the leaders are:

\begin{enumerate}
\item The first three-address instruction in the intermediate code is a
leader.
\item Any instruction that is the target of a conditional or unconditional
jump is a leader.
\item Any instruction that immediately follows a conditional or unconditional
jump is a leader.
\item The instruction just pass the end of the intermediate program is not
included as a leader.
\end{enumerate}
\item Find the basic block: for each leader, its basic block consists of
itself and all instructions up to but not including the next leader
or the end of the intermediate program. 
\end{enumerate}
Then the basic blocks become the nodes of a flow graph, whose edges
indicate which blocks can follow which others. There is an edge from
block $B$ to block $C$ if and only if it is possible for the first
instruction in block $C$ to immediately follow the last instruction
in block $B$. There are two ways that such and edge could be justified:
\begin{itemize}
\item There is a conditional or unconditional jump from the end of $B$
to the beginning of $C$.
\item $C$ immediately follows $B$ in the original order of the three-address
instructions, and $B$ does not end in an unconditional jump.
\end{itemize}
Often we add two nodes, called \emph{entry} and \emph{exit}, that
do not correspond to executable intermediate instructions.


\subsection{Liveliness and next-use}

The following algorithm determines the liveliness and next-use for
each statement in a basic block $B$.

We start at the last statement in $B$ and scan backwards to the beginning
of $B$. At each statement $i:x=y\mathtt{op}z$, we do the following:
\begin{enumerate}
\item Attach to statement $i$ the information currently found in the symbol
table regarding the next use and liveness of $x$, $y$ and $z$.
\item In the symbol table, set $x$ to {}``not live'' and {}``no next
use''
\item In the symbol table, set $y$ and $z$ to live and the next uses of
$y$ and $z$ to $i$.
\end{enumerate}
Here we have used $\mathtt{op}$ as a symbol representing an operator.
If the three-address statement $i$ is o the form $x=\mathtt{op}y$
or $x=y$, the steps are the same as above, ignoring $z$.
\end{document}
